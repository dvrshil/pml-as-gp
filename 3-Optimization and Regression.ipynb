{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b8b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "import requests\n",
    "from typing import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27e62aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3591ce",
   "metadata": {},
   "source": [
    "# Notebook 3: Optimization and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d1cfa",
   "metadata": {},
   "source": [
    "## Problem 1: A Fully Connected and Single-Layer Neural Network\n",
    "\n",
    "Suppose we are fitting a regression model to a dataset $(x_i, y_i)_{1 \\leq i \\leq N}$\n",
    "\\begin{align*}\n",
    "p(y^i|x^i; A, \\mu) & = \\mathcal{N}(f(Ax^i + \\mu), 1) \\\\\n",
    "p(y |x; A, \\mu) & = \\prod_{i=1}^N p(y^i|x^i; A, \\mu)\n",
    "\\end{align*}\n",
    "where\n",
    "1. the inputs $x^i \\in \\mathbb{R}^d$ are d-D vectors\n",
    "2. $A$ is a $1 \\times d$ matrix of weights\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "a_{11} & \\dots & a_{1d} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "3. $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is some function\n",
    "4. $x$ is a $Nxd$ matrix where row $i$ contains $x^i$\n",
    "$$\n",
    "x = \\begin{pmatrix}\n",
    "- & x^1 & -\\\\\n",
    "\\vdots & \\vdots & \\vdots\\\\\n",
    "- & x^N & -\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "5. $y$ is a vector of values to regress against\n",
    "$$\n",
    "y = \\begin{pmatrix}\n",
    "y^1 \\\\\n",
    "\\vdots \\\\\n",
    "y^N\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "6. $\\mu \\in \\mathbb{R}$ is an offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48dd24a",
   "metadata": {},
   "source": [
    "### Problem 1a\n",
    "\n",
    "Implement the conditional density:\n",
    "$$\n",
    "p(y |x; A, \\mu) = \\prod_{i=1}^N p(y^i|x^i; A, \\mu)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96a2284b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17., 26., 35., 10., 13., 16.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scout resources to verify correctness of the implementation.\n",
    "\n",
    "x = np.array([[1,2,3], [4,5,6], [7,8,9], [0,0,0], [1,1,1], [2,2,2]])\n",
    "\n",
    "A = np.array([0.25,0.5,0.75]) #True value\n",
    "Mu = np.array([5]) #True value\n",
    "\n",
    "A_rand = np.random.rand(3)\n",
    "Mu_rand = np.random.rand(1)\n",
    "\n",
    "def f(x): return 2*x\n",
    "def grad_f(x): return np.array([2])\n",
    "\n",
    "y = f(A @ x.T + Mu)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e74537c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1_density(f: Callable[[float], float],\n",
    "                   A: np.ndarray,\n",
    "                   mu: float) -> Callable[[np.ndarray, np.ndarray], float]:\n",
    "    # Inputs:\n",
    "    #     f is some function\n",
    "    #     A is a 1xd matrix\n",
    "    #     mu is a float indicating the constant offset\n",
    "    #\n",
    "    # Outputs:\n",
    "    #     Return the density as a function of the dataset x and y\n",
    "    \n",
    "    def logdensity(x: np.ndarray, y: np.ndarray) -> float:\n",
    "        # Inputs:\n",
    "        #     x is a Nxd matrix where each row is a datapoint\n",
    "        #\n",
    "        # Outputs:\n",
    "        #     y is a length n vector to evaluate the density on\n",
    "        \n",
    "        #return np.sum(np.log((density_f(x, y))))\n",
    "        \n",
    "        normals = [ stats.norm(loc = f(A @ x[i].T + mu), scale = 1) for i in range(x.shape[0]) ]\n",
    "        density_f = [ normals[i].pdf(y[i]) for i in range(len(y)) ]\n",
    "        \n",
    "        return np.prod(density_f)\n",
    "\n",
    "    return lambda x, y: logdensity(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e52a976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004031441804149938"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_density(f, A, Mu)(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9ee197",
   "metadata": {},
   "source": [
    "### Problem 1b\n",
    "\n",
    "Derive and implement\n",
    "$$\n",
    "\\frac{\\partial}{\\partial A} p(y |x; A, \\mu)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mu} p(y |x; A, \\mu) \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28d56465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import code\n",
    "\n",
    "def grad_A(f: Callable[[float], float],\n",
    "           grad_f: Callable[[float], float],\n",
    "           A: np.ndarray,\n",
    "           mu: float,\n",
    "           x: np.ndarray,\n",
    "           y: np.ndarray) -> np.ndarray:\n",
    "    \n",
    "       # Inputs:\n",
    "       #     f is some function\n",
    "       #     grad_f is the gradient of f\n",
    "       #     A is a 1xd matrix\n",
    "       #     mu is a float indicating the constant offset\n",
    "       #     x is a nxd matrix of inputs\n",
    "       #     y is a length n vector of regression outputs\n",
    "       #\n",
    "       # Outputs:\n",
    "       #     Return the gradient of the conditional density w.r.t. the matrix A, which should be a 1xd matrix.\n",
    "\n",
    "       #delA = lambda x, y: ( x * np.e**(0.5 * (y - f(A @ x + mu))**2) * (f(A @ x + mu) - y) * grad_f(A @ x + mu) ) / np.sqrt(2 * np.pi)\n",
    "       #grad = [ delA(x[i], y[i]) for i in range(x.shape[0]) ]\n",
    "\n",
    "       log_delA_xi = lambda xi, yi: -1 * xi * grad_f(A @ xi + mu) * (yi - f(A @ xi + mu))\n",
    "       log_delA = [ log_delA_xi(x[i], y[i]) for i in range(x.shape[0]) ]\n",
    "\n",
    "       #log_delA_xi = lambda xi, yi: [ xi.T @ ((yi - (f(A * xi[d] + mu))) * (grad_f(A * xi[d] + mu))) for d in range(xi.shape[0]) ]\n",
    "       #log_gradA = [ np.sum(log_delA_xi(x[i], y[i])) for i in range(x.shape[0]) ]\n",
    "\n",
    "       log_gradA = np.sum(log_delA, axis=0)\n",
    "\n",
    "       return log_gradA\n",
    "\n",
    "       pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ab93281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_mu(f: Callable[[float], float],\n",
    "            grad_f: Callable[[float], float],\n",
    "            A: np.ndarray,\n",
    "            mu: float,\n",
    "            x: np.ndarray,\n",
    "            y: np.ndarray) -> float:\n",
    "        # Inputs:\n",
    "        #     f is some function\n",
    "        #     grad_f is the gradient of f\n",
    "        #     A is a 1xd matrix\n",
    "        #     mu is a float indicating the constant offset\n",
    "        #     x is a nxd matrix of inputs\n",
    "        #     y is a length n vector of regression outputs\n",
    "        #\n",
    "        # Outputs:\n",
    "        #     Return the gradient of the conditional density w.r.t. the offset mu, which should be a float.\n",
    "\n",
    "        #delmu = lambda x, y: (np.e**(0.5 * (y - f(A @ x + mu))**2) * (f(A @ x + mu) - y) * grad_f(A @ x + mu)) / np.sqrt(2 * np.pi) #is it y or x?\n",
    "        #grad = [ delmu(x[i], y[i]) for i in range(x.shape[0]) ]\n",
    "\n",
    "        #log_delmu_xi = lambda x, y: (y - (f(A @ x + mu)) * (1 - grad_f(A @ x + mu))) #is it y or x?\n",
    "        #log_gradmu = [ log_delmu_xi(x[i], y[i]) for i in range(x.shape[0]) ]\n",
    "\n",
    "        log_delmu_xi = lambda xi, yi: -1 * grad_f(A @ xi + mu) * (yi - f(A @ xi + mu))\n",
    "        log_delmu = [ log_delmu_xi(x[i], y[i]) for i in range(x.shape[0]) ]\n",
    "\n",
    "        log_gradmu = np.sum(log_delmu, axis=0)\n",
    "\n",
    "        return log_gradmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20807c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-370.33, -446.39, -522.44]), array([-127.74]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_A(f, grad_f, A_rand, Mu_rand, x, y).round(2), grad_mu(f, grad_f, A_rand, Mu_rand, x, y).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9386f892",
   "metadata": {},
   "source": [
    "### Problem 1c\n",
    "\n",
    "Write a function that solves for the weights by finding the approximate minimum of the conditional density, i.e., solve\n",
    "$$\n",
    "\\operatorname{argmin}_{A, \\mu} -p(y | x; A, \\mu)\n",
    "$$\n",
    "with **stochastic gradient descent**. Hint: you may want to use the negative log-likelihood trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d5c3e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_for_weights1(f: Callable[[float], float],\n",
    "                      grad_f: Callable[[float], float],\n",
    "                      x: np.ndarray, \n",
    "                      y: np.ndarray,\n",
    "                      A: np.ndarray,\n",
    "                      mu: float,\n",
    "                      initial_A: np.ndarray,\n",
    "                      initial_mu: np.ndarray,\n",
    "                      step_size: float,\n",
    "                      batch_size: int) -> Tuple[np.ndarray, float]:\n",
    "    # Inputs:\n",
    "    #     f is some function\n",
    "    #     grad_f is the gradient of f\n",
    "    #     A is a 1xd matrix\n",
    "    #     mu is a float indicating the constant offset\n",
    "    #     initial_A is a 1xd matrix containing the initial guess of A for stochastic gradient descent\n",
    "    #     initial_mu is a float containing the initial guess of mu for stochastic gradient descent\n",
    "    #     step_size is a the step size of stochastic gradient descent\n",
    "    #     batch_size is the batch size of stochastic gradient descent    \n",
    "    # \n",
    "    # Outputs:\n",
    "    #     Return the density as a function of the dataset x and y\n",
    "    #     \n",
    "    #     !!do argmin of the negative log likelihood of p(y|x, A, mu)\n",
    "    \n",
    "    best_A = initial_A\n",
    "    best_mu = initial_mu\n",
    "    \n",
    "    #best_A = np.zeros((x.shape[1]))\n",
    "    #best_mu = 0\n",
    "\n",
    "    for i in range(100):\n",
    "        perm = np.random.permutation(len(y))\n",
    "        shuffled_x = x[perm]\n",
    "        shuffled_y = y[perm]\n",
    "    \n",
    "    for j in range(0, x.shape[0], batch_size):\n",
    "        \n",
    "        batch_x = shuffled_x[j*batch_size:(j+1)*batch_size,:]\n",
    "        batch_y = shuffled_y[j*batch_size:(j+1)*batch_size]\n",
    "        \n",
    "        step_A = step_size * grad_A(f, grad_f, best_A, best_mu, batch_x, batch_y)\n",
    "        step_mu = step_size * grad_mu(f, grad_f, best_A, best_mu, batch_x, batch_y)\n",
    "\n",
    "        new_density = model1_density(f, best_A - step_A, best_mu - step_mu)(batch_x, batch_y)\n",
    "        old_density = model1_density(f, best_A, best_mu)(batch_x, batch_y)\n",
    "\n",
    "        if new_density > old_density:\n",
    "            best_A = best_A - step_A\n",
    "            best_mu = best_mu - step_mu\n",
    "\n",
    "    return best_A, best_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7f54fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A from SGD: [0.09 0.29 0.49]\n",
      "Mu from SGD: [6.91]\n",
      "\n",
      "Probability Density after SGD: 3.1600389862386185e-19\n"
     ]
    }
   ],
   "source": [
    "#initial_A = np.random.rand(x.shape[1])\n",
    "#initial_mu = np.random.rand(1)\n",
    "\n",
    "#initial_A = np.zeros((x.shape[1]))\n",
    "#initial_mu = 0\n",
    "\n",
    "initial_A = np.array([0.5, 0.75, 1])\n",
    "initial_mu = 7\n",
    "\n",
    "step_size = 2e-3\n",
    "batch_size = 1\n",
    "\n",
    "gd_A, gd_Mu = solve_for_weights1(f, grad_f, x, y, A, Mu, initial_A, initial_mu, step_size, batch_size)\n",
    "\n",
    "print(\"A from SGD:\" , gd_A.round(2))\n",
    "print(\"Mu from SGD:\" , gd_Mu.round(2))\n",
    "\n",
    "print(\"\\nProbability Density after SGD:\" , model1_density(f, gd_A, gd_Mu)(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c402d3b5",
   "metadata": {},
   "source": [
    "## Problem 2: A Fully Connected and Two-Layer Neural Network\n",
    "\n",
    "Suppose we are fitting a regression model to a dataset $(x_i, y_i)_{1 \\leq i \\leq N}$\n",
    "\\begin{align*}\n",
    "p(y^i|x^i; A_1, \\mu_1, A_2, \\mu_2) & = \\mathcal{N}(f(A_2 f(A_1x^i + \\mu_1) + \\mu_2), 1) \\\\\n",
    "p(y |x; A_1, \\mu_1, A_2, \\mu_2) & = \\prod_{i=1}^N p(y^i|x^i; A_1, \\mu_1, A_2, \\mu_2)\n",
    "\\end{align*}\n",
    "where\n",
    "1. the inputs $x^i \\in \\mathbb{R}^d$ are d-D vectors\n",
    "2. $A_1$ is a $m \\times d$ matrix of weights and $A_2$ is a $1 \\times m$ matrix of weights.\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "a_{11} & \\dots & a_{1d} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "3. $\\mu_1 \\in \\mathbb{R}^m$ is a vector of weights and $\\mu_2 \\in \\mathbb{R}$ is a weight.\n",
    "4. $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is some function\n",
    "5. $x$ is a $Nxd$ matrix where row $i$ contains $x^i$\n",
    "$$\n",
    "x = \\begin{pmatrix}\n",
    "- & x^1 & -\\\\\n",
    "\\vdots & \\vdots & \\vdots\\\\\n",
    "- & x^N & -\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "6. $y$ is a vector of values to regress against\n",
    "$$\n",
    "y = \\begin{pmatrix}\n",
    "y^1 \\\\\n",
    "\\vdots \\\\\n",
    "y^N\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe9f7c5",
   "metadata": {},
   "source": [
    "### Problem 2a\n",
    "\n",
    "Implement the conditional density:\n",
    "$$\n",
    "p(y |x; A_1, \\mu_1, A_2, \\mu_2) = \\prod_{i=1}^N p(y^i|x^i; A_1, \\mu_1, A_2, \\mu_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5e8d719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26.6],\n",
       "       [51.8],\n",
       "       [77. ],\n",
       "       [ 7.4],\n",
       "       [15.8],\n",
       "       [24.2]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scout resources to verify correctness of the implementation.\n",
    "\n",
    "x = np.array([[1,2,3], [4,5,6], [7,8,9], [0,0,0], [1,1,1], [2,2,2]])\n",
    "\n",
    "A1 = np.array([[0.25,0.5,0.75], [0.1, 0.2, 0.3], [0.2, 0.3, 0.4]]) #True value, m = 3\n",
    "A2 = np.array([0.5,0.75,1]) #True value\n",
    "A1_rand = np.random.rand(3,3)\n",
    "A2_rand = np.random.rand(3)\n",
    "\n",
    "Mu1 = np.array([0.5,0.6,0.7]) #True value\n",
    "Mu2 = np.array([0.9]) #True value\n",
    "Mu1_rand = np.random.rand(3)\n",
    "Mu2_rand = np.random.rand(1)\n",
    "\n",
    "def f(x): return 2*x\n",
    "def grad_f(x): return np.array([2])\n",
    "\n",
    "y = np.asarray([ f(A2 @ f(A1 @ x[i] + Mu1) + Mu2) for i in range(x.shape[0]) ])\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1fbf9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2_density(f: Callable[[float], float],\n",
    "                   A1: np.ndarray,\n",
    "                   mu1: float,\n",
    "                   A2: np.ndarray,\n",
    "                   mu2: float) -> Callable[[np.ndarray, np.ndarray], float]:\n",
    "    # Inputs:\n",
    "    #     f is some function\n",
    "    #     A1 is a mxd matrix\n",
    "    #     mu1 is a length m vector\n",
    "    #     A2 is a 1xm matrix\n",
    "    #     mu2 is a float indicating the constant offset\n",
    "    #\n",
    "    # Outputs:\n",
    "    #     Return the density as a function of the dataset x and y\n",
    "    \n",
    "    def density(x: np.ndarray, y: np.ndarray) -> float:\n",
    "        # Inputs:\n",
    "        #     x is a Nxd matrix where each row is a datapoint\n",
    "        #\n",
    "        # Outputs:\n",
    "        #     y is a length n vector to evaluate the density on\\\n",
    "\n",
    "        normals = [ stats.norm(loc=f(A2 @ f(A1 @ x[i] + mu1) + mu2), scale = 1) for i in range(x.shape[0]) ]\n",
    "        density_f = [ normals[i].pdf(y[i]) for i in range(len(y)) ]\n",
    "        \n",
    "        return np.prod(density_f)\n",
    "\n",
    "    return lambda x,y: density(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e24e68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004031441804149938"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2_density(f, A1, Mu1, A2, Mu2)(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34641d3",
   "metadata": {},
   "source": [
    "### Problem 2b\n",
    "\n",
    "Derive and implement\n",
    "$$\n",
    "\\frac{\\partial}{\\partial A_1} p(y |x; A_1, \\mu_1, A_2, \\mu_2)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mu_1} p(y |x; A_1, \\mu_1, A_2, \\mu_2) \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24626e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_A1(f: Callable[[float], float],\n",
    "            grad_f: Callable[[float], float],\n",
    "            A1: np.ndarray,\n",
    "            mu1: float,\n",
    "            A2: np.ndarray,\n",
    "            mu2: float,\n",
    "            x: np.ndarray,\n",
    "            y: np.ndarray) -> np.ndarray:\n",
    "        # Inputs:\n",
    "        #     f is some function\n",
    "        #     grad_f is the gradient of f\n",
    "        #     A1 is a mxd matrix of weights\n",
    "        #     mu1 is a vector of constant offsets\n",
    "        #     A2 is a 1xm matrix of weights\n",
    "        #     mu2 is a float that is a constant offset\n",
    "        #     x is a nxd matrix of inputs\n",
    "        #     y is a length n vector of regression outputs\n",
    "        #\n",
    "        # Outputs:\n",
    "        #     Return the gradient of the conditional density w.r.t. the matrix A, which should be a 1xd matrix.\n",
    "\n",
    "        log_delA1_xi = lambda xi, yi: -1 * xi * (A2 * grad_f(A1 @ xi + Mu1) * grad_f(A2 @ f(A1 @ xi + Mu1) + Mu2) * (yi - f(A2 @ f(A1 @ xi + Mu1) + Mu2)))\n",
    "        log_delA1 = [ log_delA1_xi(x[i], y[i]) for i in range(x.shape[0]) ]\n",
    "        \n",
    "        log_gradA1 = np.sum(log_delA1, axis = 0)\n",
    "\n",
    "        return log_gradA1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38d1b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_mu1(f: Callable[[float], float],\n",
    "            grad_f: Callable[[float], float],\n",
    "            A1: np.ndarray,\n",
    "            mu1: float,\n",
    "            A2: np.ndarray,\n",
    "            mu2: float,\n",
    "            x: np.ndarray,\n",
    "            y: np.ndarray) -> np.ndarray:\n",
    "        # Inputs:\n",
    "        #     f is some function\n",
    "        #     grad_f is the gradient of f\n",
    "        #     A1 is a mxd matrix of weights\n",
    "        #     mu1 is a vector of constant offsets\n",
    "        #     A2 is a 1xm matrix of weights\n",
    "        #     mu2 is a float that is a constant offset\n",
    "        #     x is a nxd matrix of inputs\n",
    "        #     y is a length n vector of regression outputs\n",
    "        #\n",
    "        # Outputs:\n",
    "        #     Return the gradient of the conditional density w.r.t. the matrix A, which should be a 1xd matrix.\n",
    "\n",
    "        log_delmu1_xi = lambda xi, yi: -1 * A2 * grad_f(A1 @ xi + Mu1) * grad_f(A2 @ f(A1 @ xi + Mu1) + Mu2) * (yi - f(A2 @ f(A1 @ xi + Mu1) + Mu2))\n",
    "        log_delmu1 = [ log_delmu1_xi(x[i], y[i]) for i in range(x.shape[0]) ]\n",
    "        \n",
    "        log_gradmu1 = np.sum(log_delmu1, axis = 0)\n",
    "\n",
    "        return log_gradmu1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6aae6169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([528.19, 718.31, 653.99]), array([104.15, 121.32,  96.59]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_A1(f, grad_f, A1_rand, Mu1_rand, A2_rand, Mu2_rand, x, y).round(2), grad_mu1(f, grad_f, A1_rand, Mu1_rand, A2_rand, Mu2_rand, x, y).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e0552",
   "metadata": {},
   "source": [
    "### Problem 2c\n",
    "\n",
    "Derive and implement\n",
    "$$\n",
    "\\frac{\\partial}{\\partial A_2} p(y |x; A_1, \\mu_1, A_2, \\mu_2)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mu_2} p(y |x; A_1, \\mu_1, A_2, \\mu_2) \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "122c1f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_A2(f: Callable[[float], float],\n",
    "            grad_f: Callable[[float], float],\n",
    "            A1: np.ndarray,\n",
    "            mu1: float,\n",
    "            A2: np.ndarray,\n",
    "            mu2: float,\n",
    "            x: np.ndarray,\n",
    "            y: np.ndarray) -> np.ndarray:\n",
    "        # Inputs:\n",
    "        #     f is some function\n",
    "        #     grad_f is the gradient of f\n",
    "        #     A1 is a mxd matrix of weights\n",
    "        #     mu1 is a vector of constant offsets\n",
    "        #     A2 is a 1xm matrix of weights\n",
    "        #     mu2 is a float that is a constant offset\n",
    "        #     x is a nxd matrix of inputs\n",
    "        #     y is a length n vector of regression outputs\n",
    "        #\n",
    "        # Outputs:\n",
    "        #     Return the gradient of the conditional density w.r.t. the matrix A, which should be a 1xd matrix.\n",
    "\n",
    "        log_delA2_xi = lambda xi, yi: -1 * f(A1 @ xi + Mu1 * grad_f(A2 @ f(A1 @ xi + Mu1) + Mu2) * (yi - f(A2 @ f(A1 @ xi + Mu1) + Mu2)))\n",
    "        log_delA2 = [ log_delA2_xi(x[i], y[i]) for i in range(x.shape[0]) ]\n",
    "        \n",
    "        log_gradA2 = np.sum(log_delA2, axis = 0)\n",
    "\n",
    "        return log_gradA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f071e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_mu2(f: Callable[[float], float],\n",
    "             grad_f: Callable[[float], float],\n",
    "             A1: np.ndarray,\n",
    "             mu1: float,\n",
    "             A2: np.ndarray,\n",
    "             mu2: float,\n",
    "             x: np.ndarray,\n",
    "             y: np.ndarray) -> float:\n",
    "    # Inputs:\n",
    "    #     f is some function\n",
    "    #     grad_f is the gradient of f\n",
    "    #     A1 is a mxd matrix of weights\n",
    "    #     mu1 is a vector of constant offsets\n",
    "    #     A2 is a 1xm matrix of weights\n",
    "    #     mu2 is a float that is a constant offset\n",
    "    #     x is a nxd matrix of inputs\n",
    "    #     y is a length n vector of regression outputs\n",
    "    #\n",
    "    # Outputs:\n",
    "    #     Return the gradient of the conditional density w.r.t. the matrix A, which should be a 1xd matrix.\n",
    "\n",
    "    log_delmu2_xi = lambda xi, yi: -1 * grad_f(A2 @ f(A1 @ xi + Mu1) + Mu2) * (yi - f(A2 @ f(A1 @ xi + Mu1) + Mu2))\n",
    "    log_delmu2 = [ log_delmu2_xi(x[i], y[i]) for i in range(x.shape[0]) ]\n",
    "    \n",
    "    log_gradmu2 = np.sum(log_delmu2, axis = 0)\n",
    "\n",
    "    return log_gradmu2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "263db34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([13.62, 69.68, 55.05]), array([85.84]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_A2(f, grad_f, A1_rand, Mu1_rand, A2_rand, Mu2_rand, x, y).round(2), grad_mu2(f, grad_f, A1_rand, Mu1_rand, A2_rand, Mu2_rand, x, y).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8068a990",
   "metadata": {},
   "source": [
    "### Problem 2d\n",
    "\n",
    "Write a function that solves for the weights by finding the approximate minimum of the conditional density, i.e., solve\n",
    "$$\n",
    "\\operatorname{argmin}_{A_1, \\mu_1, A_2, \\mu_2} -p(y | x; A_1, \\mu_1, A_2, \\mu_2)\n",
    "$$\n",
    "with **stochastic gradient descent**. Hint: you may want to use the negative log-likelihood trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05064f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_for_weights2(f: Callable[[float], float],\n",
    "                      grad_f: Callable[[float], float],\n",
    "                      x: np.ndarray,\n",
    "                      y: np.ndarray,\n",
    "                      A1: np.ndarray,\n",
    "                      mu1: np.ndarray,\n",
    "                      A2: np.ndarray,\n",
    "                      mu2: float,\n",
    "                      initial_A1: np.ndarray,\n",
    "                      initial_mu1: np.ndarray,\n",
    "                      initial_A2: np.ndarray,\n",
    "                      initial_mu2: float,\n",
    "                      step_size: float,\n",
    "                      batch_size: int) -> Tuple[np.ndarray, float]:\n",
    "    # Inputs:\n",
    "    #     f is some function\n",
    "    #     grad_f is the gradient of f\n",
    "    #     A1 is a mxd matrix of weights\n",
    "    #     mu1 is a vector of constant offsets\n",
    "    #     A2 is a 1xm matrix of weights\n",
    "    #     mu2 is a float that is a constant offset\n",
    "    #     initial_A1 is a mxd matrix containing the initial guess of A1 for stochastic gradient descent\n",
    "    #     initial_mu1 is a length d vector containing the initial guess of mu1 for stochastic gradient descent\n",
    "    #     initial_A2 is a 1xm matrix containing the initial guess of A2 for stochastic gradient descent\n",
    "    #     initial_mu2 is a float containing the initial guess of mu2 for stochastic gradient descent\n",
    "    #     step_size is a the step size of stochastic gradient descent\n",
    "    #     batch_size is the batch size of stochastic gradient descent    \n",
    "    #\n",
    "    # Outputs:\n",
    "    #     Return the density as a function of the dataset x and y\n",
    "\n",
    "    best_A1 = initial_A1\n",
    "    best_mu1 = initial_mu1\n",
    "    best_A2 = initial_A2\n",
    "    best_mu2 = initial_mu2\n",
    "\n",
    "    for i in range(100):\n",
    "    # 1. Shuffle the dataset randomly\n",
    "        perm2 = np.random.permutation(len(y))\n",
    "        shuffled_x = x[perm2]\n",
    "        shuffled_y = y[perm2]\n",
    "    \n",
    "    # 2. Break the dataset into batches of size batch_size\n",
    "    for j in range(0, x.shape[0], batch_size):\n",
    "        \n",
    "        # 3. Create a batch\n",
    "        batch_x = shuffled_x[j*batch_size:(j+1)*batch_size,:]\n",
    "        batch_y = shuffled_y[j*batch_size:(j+1)*batch_size]\n",
    "        \n",
    "        # 4. Perform gradient descent using a batch\n",
    "        step_A1 = step_size * grad_A1(f, grad_f, best_A1, best_mu1, best_A2, best_mu2, batch_x, batch_y)\n",
    "        step_mu1 = step_size * grad_mu1(f, grad_f, best_A1, best_mu1, best_A2, best_mu2, batch_x, batch_y)\n",
    "        step_A2 = step_size * grad_A2(f, grad_f, best_A1, best_mu1, best_A2, best_mu2, batch_x, batch_y)\n",
    "        step_mu2 = step_size * grad_mu2(f, grad_f, best_A1, best_mu1, best_A2, best_mu2, batch_x, batch_y)\n",
    "\n",
    "        new_density = model2_density(f, best_A1 - step_A1, best_mu1 - step_mu1, best_A2 - step_A2, best_mu2 - step_mu2)(x, y)\n",
    "        current_density = model2_density(f, best_A1, best_mu1, best_A2, best_mu2)(x, y)\n",
    "\n",
    "        if new_density > current_density:\n",
    "            best_A1 = best_A1 - step_A1\n",
    "            best_mu1 = best_mu1 - step_mu1\n",
    "            best_A2 = best_A2 - step_A2\n",
    "            best_mu2 = best_mu2 - step_mu2\n",
    "    \n",
    "    return best_A1, best_mu1, best_A2, best_mu2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06558fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A1 from SGD: [[0.56 0.74 0.95]\n",
      " [0.31 0.44 0.75]\n",
      " [0.66 0.84 1.05]]\n",
      "Mu1 from SGD: [0.82 0.94 1.06]\n",
      "A2 from SGD: [0.22 0.33 0.46]\n",
      "Mu2 from SGD: [1.09]\n",
      "\n",
      "Probability Density after SGD: 0.00010606986048196353\n"
     ]
    }
   ],
   "source": [
    "step_size = 1e-3\n",
    "batch_size = 1\n",
    "\n",
    "initial_A1 = np.array([[0.5,0.6,0.7], [0.25, 0.3, 0.5], [0.6, 0.7, 0.8]]) #True value, m = 3\n",
    "initial_A2 = np.array([0.1,0.2,0.3]) #True value\n",
    "initial_Mu1 = np.array([0.8,0.9,1]) #True value\n",
    "initial_Mu2 = np.array([1]) #True value\n",
    "\n",
    "gdA1, gdMu1, gdA2, gdMu2 = solve_for_weights2(f, grad_f, x, y, A1, Mu1, A2, Mu2, initial_A1, initial_Mu1, initial_A2, initial_Mu2, step_size, batch_size)\n",
    "\n",
    "print(\"\\nA1 from SGD:\" , gdA1.round(2))\n",
    "print(\"Mu1 from SGD:\" , gdMu1.round(2))\n",
    "print(\"A2 from SGD:\" , gdA2.round(2))\n",
    "print(\"Mu2 from SGD:\" , gdMu2.round(2))\n",
    "\n",
    "print(\"\\nProbability Density after SGD:\" , model2_density(f, gdA1, gdMu1, gdA2, gdMu2)(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f134a92",
   "metadata": {},
   "source": [
    "### Problem 3: Multi-output Regression\n",
    "\n",
    "Suppose we are fitting a regression model to a dataset $(x_i, y_i)_{1 \\leq i \\leq N}$\n",
    "\\begin{align*}\n",
    "p(y^i|x^i; A, \\mu, \\Sigma) & = \\mathcal{N}(f(Ax^i + \\mu), \\Sigma) \\\\\n",
    "p(y |x; A, \\mu, \\Sigma) & = \\prod_{i=1}^N p(y^i|x^i; A, \\mu, \\Sigma)\n",
    "\\end{align*}\n",
    "where\n",
    "1. the inputs $x^i \\in \\mathbb{R}^d$ are d-D vectors\n",
    "2. $A$ is a $2 \\times d$ matrix of weights\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "a_{11} & \\dots & a_{1d} \\\\\n",
    "a_{21} & \\dots & a_{2d}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "3. $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is some function\n",
    "4. $x$ is a $Nxd$ matrix where row $i$ contains $x^i$\n",
    "$$\n",
    "x = \\begin{pmatrix}\n",
    "- & x^1 & -\\\\\n",
    "\\vdots & \\vdots & \\vdots\\\\\n",
    "- & x^N & -\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "5. $y$ is a $Nx2$ matrix of values to regress against\n",
    "$$\n",
    "y = \\begin{pmatrix}\n",
    "y^1_1 & y^1_2\\\\\n",
    "\\vdots \\\\\n",
    "y^N_1 & y^N_2\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "6. $\\Sigma$ is a covariance matrix\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\sigma_{11} & \\sigma_{12} \\\\\n",
    "\\sigma_{21} & \\sigma_{22}\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db5d7d",
   "metadata": {},
   "source": [
    "### Problem 3a\n",
    "\n",
    "Suppose we are solving for\n",
    "$$\n",
    "\\operatorname{argmin}_{A, \\mu} -p(y | x; A, \\mu, \\Sigma) \\,.\n",
    "$$\n",
    "\n",
    "Derive the associated loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3b290d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# product of all the normal densities\n",
    "\n",
    "def model3_density(f: Callable[[float], float],\n",
    "                   A: np.ndarray,\n",
    "                   sigma: np.ndarray,\n",
    "                   mu: np.ndarray) -> Callable[[np.ndarray, np.ndarray], float]:\n",
    "    \n",
    "    def density(x: np.ndarray, y: np.ndarray) -> float:\n",
    "\n",
    "        mvnorms = [ stats.multivariate_normal(mean=f(A @ x[i].T + mu), cov=sigma) for i in range(len(y)) ]\n",
    "        density_f = [ mvnorms[i].pdf(y[i]) for i in range(y.shape[0]) ]\n",
    "        \n",
    "        return np.prod(density_f, axis=0)\n",
    "\n",
    "    return lambda x,y: density(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5712479",
   "metadata": {},
   "source": [
    "### Problem 3b\n",
    "\n",
    "Compare and contrast the situation when $\\Sigma$ is an identity matrix and when it is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "598052fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20600476, 0.29424385],\n",
       "       [0.29424385, 1.2242376 ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu = np.array([0.,0]) #True value\n",
    "identity = np.eye(2)\n",
    "\n",
    "L = np.array([[sp.stats.norm().rvs(), 0.0], [sp.stats.norm().rvs(), sp.stats.norm().rvs()]])\n",
    "sigma = L @ L.transpose()\n",
    "\n",
    "x = np.array([[1,3], [3,5], [5,7], [7,9], [9,11]])\n",
    "#y = np.array([2,4,6,8,10])\n",
    "\n",
    "A = np.array([[0.1,0.2], [0.2, 0.3]])\n",
    "\n",
    "def f(x): return 2*x\n",
    "\n",
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "41313266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.06744104,  3.26690482],\n",
       "        [ 2.99861857,  4.93275506],\n",
       "        [ 2.32434514,  6.13683571],\n",
       "        [ 5.63531998,  8.12180746],\n",
       "        [ 4.45093891, 10.04339148]]),\n",
       " array([[1.70943973, 1.96319819],\n",
       "        [2.42805384, 4.26902711],\n",
       "        [4.16174029, 5.56301653],\n",
       "        [4.36007953, 7.79341432],\n",
       "        [6.33974817, 9.83586476]]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_distribution = [ stats.multivariate_normal(mean= f(A @ x[i].T), cov=identity) for i in range(x.shape[0]) ]\n",
    "ys1 = np.array([ id_distribution[i].rvs(size=1) for i in range(x.shape[0]) ])\n",
    "\n",
    "nonid_distribution = [ stats.multivariate_normal(mean= f(A @ x[i].T), cov=sigma) for i in range(x.shape[0]) ]\n",
    "ys2 = np.array([ nonid_distribution[i].rvs(size=1) for i in range(x.shape[0]) ])\n",
    "\n",
    "ys1, ys2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c769196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.145337377632025e-07, 0.0003741124346843708)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3_density(f, A, identity, mu)(x, ys1), model3_density(f, A, sigma, mu)(x, ys2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1dab99",
   "metadata": {},
   "source": [
    "An identity covariance would have a symettric distribution over all the dimension w.r.t. the mean vector.\n",
    "\n",
    "The non-identity covariance matrix leads to a skewed distribution. This leads to a more/less dense distribution around the mean.\n",
    "\n",
    "A non-identity covariance matrix is a matrix where the variances are not all the same. This means that the variables are not all independent of each other. While an identity covariance means all the variables are idependent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
